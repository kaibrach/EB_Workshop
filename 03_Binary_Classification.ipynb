{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some small Classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkwbsZYtY7_7"
   },
   "source": [
    "## 1. Banknote classification with fully connected neural networks\n",
    "\n",
    "**Goal:** In this notebook you will do your first classification. You will see that fully connected networks without a hidden layer can only learn linar decision boundaries, while fully connected networks with hidden layers are able to learn non-linear decision boundaries.\n",
    "\n",
    "**Usage:** The idea of the notebook is that you try to understand the provided code. Run it, check the output, and play with it by slightly changing the code. \n",
    "\n",
    "**Dataset:** You work with a banknote data set and classification task. We have 5 features of wavelet transformed images of banknotes:\n",
    ">1. variance  (continuous feature) \n",
    ">2. skewness (continuous feature) \n",
    ">3. curtosis (continuous feature) \n",
    ">4. entropy (continuous feature) \n",
    ">5. class (binary indicating if the banknote is real or fake)  \n",
    "\n",
    "Don't bother too much how these features exactely came from.\n",
    "\n",
    "For this analysis we only use 2 features. \n",
    "\n",
    ">x1: skewness of wavelet transformed image  \n",
    ">x2: entropy of wavelet transformed image\n",
    "\n",
    "\n",
    "**The goal is to classify each banknote to either \"real\" (Y=0) or \"fake\" (Y=1).**\n",
    "\n",
    "\n",
    "**Content:**\n",
    "* visualize the data in a simple scatter plot and color the points by the class label\n",
    "* use the Keras library to build a fcNN without hidden layers (logistic regression). Use SGD with the objective to minimize the crossentropy loss. \n",
    "* visualize the learned decision boundary in a 2D plot\n",
    "* use the Keras library to build a fcNN with a single hidden layer. Use SGD with the objective to minimize the crossentropy loss. \n",
    "* visualize the learned decision boundary in a 2D plot\n",
    "* compare the performace and the decision boundaries of the two models\n",
    "* stack more hidden layers to the model and playaround with the epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlIPKjGu-UEj"
   },
   "source": [
    "#### Imports\n",
    "\n",
    "In the next two cells, we load all the required libraries and functions from keras and numpy. We also download the data with the 5 featues from the provided url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJSZXo7S-H5n"
   },
   "outputs": [],
   "source": [
    "# load required libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6Ofvh7fPYV9i",
    "outputId": "41501494-fe0e-4b4b-e135-34f1a162ad0d"
   },
   "outputs": [],
   "source": [
    "# Load data from url\n",
    "from urllib.request import urlopen\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n",
    "raw_data = urlopen(url)\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlDPWop1_zGM"
   },
   "source": [
    "Let's extract the two featues *x1: skewness of wavelet transformed image* and *x2: entropy of wavelet transformed image*. We print the shape and see that we for X  we have 1372 oberservations with two featues and for Y there are 1372 binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "OHixTIf3fBFL",
    "outputId": "0ceca60e-52f9-4e3b-8d0c-e17fcbef0faa"
   },
   "outputs": [],
   "source": [
    "# Here we use extract the two features and the label of the dataset\n",
    "X=dataset[:,[1,3]] # Extract skewness and entropy\n",
    "Y=dataset[:,4]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2upXCjUBweV"
   },
   "source": [
    "Since the banknotes are described by only 2 features, we can easily visualize the positions of real and fake banknotes in the 2D feature space. You can see that the boundary between the two classes is not separable by a straight line. A curved boundary line will do better. But even then we cannot expect a perfect seperation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "o91QiD0Ij8E8",
    "outputId": "58bbe7cf-44f4-430c-daa6-f87da7825f56"
   },
   "outputs": [],
   "source": [
    "# visualize the data in a 2D feature space. \n",
    "idx_f = [np.where(Y==1)]\n",
    "idx_r = [np.where(Y==0)]\n",
    "params = {'mathtext.default': 'regular' }  #Nicer Plotting (Latex Style)\n",
    "plt.rcParams.update(params)\n",
    "plt.scatter(X[idx_r,0],X[idx_r,1], alpha=1.0,marker='^',edgecolor='black')\n",
    "plt.scatter(X[idx_f,0],X[idx_f,1], alpha=1.0,marker='o',edgecolor='black')\n",
    "plt.title(\"Real and faked banknotes\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.legend((\"faked\",\"real\"),\n",
    "           loc='lower left',\n",
    "           fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXMTljrqtXqt"
   },
   "source": [
    "## fcNN with only one neuron\n",
    "Let’s try to use a single neuron with a sigmoid activation function (also known as logistic regression) as classification model to seperate the banknotes.  \n",
    "We use the sequential API from keras to build the model. To fit the 3 parameters we use the stochastic gradient descent optimizer with a learning rate of 0.15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64rucrOKe2mW"
   },
   "source": [
    "### Definition of a NN with only one neuron after the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsfJWZi6gc04"
   },
   "outputs": [],
   "source": [
    "# Definition of the network\n",
    "model = Sequential()                                        # starts the definition of the network\n",
    "model.add(Dense(1, batch_input_shape=(None, 2),             # adds a new layer to the network with a single neuron  \n",
    "                activation='sigmoid'))                      # The input is a tensor of size (batch_size, 2), since we don’t specify the Batch Size now, we use None as a placeholder\n",
    "                                                            # chooses the activation function ‘sigmoid’\n",
    "# Definition of the optimizer\n",
    "sgd = optimizers.SGD(lr=0.15)                               # Defining the stochastic gradient descent optimizer\n",
    "\n",
    "# compile model                                             # compile model, which ends the definition of the model \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,                                # using the stochastic gradient descent optimizer\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "9ZfkGjyjgneX",
    "outputId": "d9a77785-e57a-4dd0-d51f-484609d432f6"
   },
   "outputs": [],
   "source": [
    "# summarize the architecture of the NN along with the number of weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AI_YeSyBEHTc"
   },
   "source": [
    "In the next cell, we train the network. In other words, we tune the parameters that were initialized randomly with stochastic gradient descent to minimize our loss function (the binary corssentropy). We set the batchsize to 128 per updatestep and train for 400 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E34YcE-Wg34o"
   },
   "outputs": [],
   "source": [
    "# Training of the network\n",
    "history = model.fit(X, Y,                           # training of the model using the training data stored in X and Y for 4100 epochs\n",
    "          epochs=400,                               # for 400 epochs\n",
    "          batch_size=128,                           # fix the batch size to 128 examples\n",
    "          verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqE0FrJ9FPM2"
   },
   "source": [
    "Let's look at the so called leraning curve, we plot the accuracy and the loss vs the epochs. You can see that after 100 epochs, we predict around 70% of our data correct and have a loss aorund 0.51 (this values can vary from run to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "colab_type": "code",
    "id": "vFQ5RgbciCqc",
    "outputId": "4dedf3ce-ee95-44e9-da7a-31388f020337"
   },
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and the loss during the training\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='lower right')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJxQApCTsSK1"
   },
   "source": [
    "### Plotting the learned decision boundary\n",
    "Let's visualize which decision boundary was learned by the fcNN with only one output neuron (and no hidden layer).  \n",
    "As you can see the decision boundary is a straight line. This is not a coincidence but a general property of a single artificial neuron with a sigmoid as activation function and no hidden layer, also known as logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "DrWqT9T2oStj",
    "outputId": "430187f7-0685-4aac-eff4-45bc181f7b2f"
   },
   "outputs": [],
   "source": [
    "def plotModel(X,Y, model, t):\n",
    "    # define a grid for the 2D feature space\n",
    "    # predict at each grid point the probability for class 1\n",
    "\n",
    "    x1list = np.linspace(np.min(X[:,0])-2, np.max(X[:,0])+2, 20) # Define 50 points on the x-axis\n",
    "    x2list = np.linspace(np.min(X[:,1])-2, np.max(X[:,1])+2, 20) # Define 50 points on the x-axis\n",
    "    X1_grid, X2_grid = np.meshgrid(x1list, x2list)\n",
    "\n",
    "    # model.predict for respective value x1 and x2 \n",
    "    p = np.array([model.predict(np.reshape(np.array([l1,l2]),(1,2))) for l1,l2 in zip(np.ravel(X1_grid), np.ravel(X2_grid))])\n",
    "    print(p.shape)\n",
    "    if len(p.shape) == 3 and p.shape[2]==2:\n",
    "        p = p[:,:,1] # pick p for class 1 if there are more than 2 classes\n",
    "    p = np.reshape(p,X1_grid.shape)\n",
    "\n",
    "    # visualize the predicted probabilities in the 2D feature space\n",
    "    # once without and once with the data points used for fitting\n",
    "    params = {'mathtext.default': 'regular' }  #Nicer Plotting\n",
    "    plt.rcParams.update(params)\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(1,2,(1))\n",
    "    cp = plt.contourf(X1_grid, X2_grid, p,cmap='viridis')\n",
    "    plt.colorbar(cp)\n",
    "    plt.title(t)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "\n",
    "    plt.subplot(1,2,(2))\n",
    "    cp = plt.contourf(X1_grid, X2_grid, p,cmap='viridis')\n",
    "    plt.colorbar(cp)\n",
    "    idx_f = [np.where(Y==1)]\n",
    "    idx_r = [np.where(Y==0)]\n",
    "    plt.scatter(X[idx_r,0],X[idx_r,1], alpha=1.0,marker='^',edgecolor='black')\n",
    "    plt.scatter(X[idx_f,0],X[idx_f,1], alpha=1.0,marker='o',edgecolor='black')\n",
    "    plt.title(t)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "\n",
    "plotModel(X, Y, model, 'fcnn separation without hidden layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BVHMfQjr8LI"
   },
   "source": [
    "## fcNN with one hidden layer \n",
    "\n",
    "We know that the boundary between the two classes is not descriped very good by a line. Therefore a single neuron is not appropriate to model the probability for a fake banknote based on its two features. To get a more flexible model, we introduce an additional layer between input layer and output layer. This is called hidden layer. Here we use a hidden layer with 8 neurons. We also change the ouputnodes form 1 to 2, to get two ouputs for the probability of real and fake banknote. Because we now have 2 outputs, we use the *softmax* activation function in the output layer. The softmax activation ensures that the output can be interpreted as a probability (see book for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ar_CkKTfO3w"
   },
   "source": [
    "### Definition of the network with two hidden layers                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgMpaj2hiHNX"
   },
   "outputs": [],
   "source": [
    "# Definition of the network\n",
    "model = Sequential()\n",
    "model.add(Dense(8, batch_input_shape=(None, 2),activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aY3XX2qHOQQz"
   },
   "source": [
    "In this is output summary we see that we now have a lot more trainable paramters then before.  \n",
    "24 = inputdim · outpuntdim + outputbias= 2 · 8 + 8   \n",
    "18 = inputdim · outpuntdim + outputbias= 8 · 2 + 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "QZ09IBXFidZQ",
    "outputId": "32400b09-1f3e-47b7-fec2-53e02177f578"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ztYP_-CIivRF",
    "outputId": "8f5d9fab-1234-4e4a-de25-e498a6a698d4"
   },
   "outputs": [],
   "source": [
    "# Transforms Y=0 to (1,0) and Y=1 to (0,1)\n",
    "Y_c=to_categorical(Y,2)\n",
    "Y[0:5], Y_c[0:5],Y[-5:-1],Y_c[-5:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Soz6r8cGRAFh"
   },
   "source": [
    "In the next cell, train the network. In other words, we tune the parameters that were initialized randomly with stochastic gradient descent to minimize our loss function (the categorical crossentropy). We set the batchsize to 128 per updatestep and train for 400 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fes2yosihh5"
   },
   "outputs": [],
   "source": [
    "# Training of the network\n",
    "history = model.fit(X, Y_c, \n",
    "          epochs=400, \n",
    "          batch_size=128,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-w1vdq-R0Wv"
   },
   "source": [
    "Let's look again at the leraning curve, we plot the accuracy and the loss vs the epochs. You can see that after 100 epochs, we predict around 86% of our data correct and have a loss aorund 0.29 (this values can vary from run to run). This is already alot better than the model without a hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "colab_type": "code",
    "id": "X1iTLPAoth6f",
    "outputId": "fe4f31ed-b82c-47da-c892-cccc1ab32115"
   },
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and loss during the training\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='lower right')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIoeyms8teNs"
   },
   "source": [
    "### Plotting the learned decision boundary\n",
    "Let's visualize which decision boundary was learned by the fcNN with the hidden layer\n",
    "As you can see the decision boundary is a now curved and not straight anymore. The model (with the hidden layer in the middle) separates the the two classes in the training data better and is able to learn non-linear decision boundaries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "mg7PikB6sP2R",
    "outputId": "19da6831-0f7c-40f5-c3c5-d2c6b6f69b07"
   },
   "outputs": [],
   "source": [
    "plotModel(X,Y,model, 'fcnn separation with hidden layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOX7awYjsWJb"
   },
   "source": [
    "#### Add more hidden layers and play around with the training epochs\n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "       1. Add more hidden layers to the model and play around with the training epochs. What do you observe? Look at the learned decision boundary. How does the loss and the accuracy change?\n",
    "       2. Use different features for classification. What do you think which features are best to classify real and fake banknotes?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4K8Ug6ICkRtQ"
   },
   "source": [
    "# 2. A simple CNN for the edge lover task\n",
    "\n",
    "**Goal:** In this notebook you train a very simple CNN with only 1 kernel to discriminate images containing vertical from those containing horizontal stripes. To check what pattern is recognized by the learned kernel you will visualize the weights of the kernel as an image. You will see that the CNN leans a useful kernel (either a vertical or horiziontal bar).You can experiment with the code to check the influence of the kernel size, the activation function and the pooling method on the result.  \n",
    "\n",
    "**Usage:** The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it.  \n",
    "\n",
    "**Dataset:** You work with an artficially generatet dataset of greyscale images (50x50 pixel) with 10 vertical or horizontal bars. We want to classify them into whether or not an art lover will like them(0 or 1).  \n",
    "\n",
    "**Content:**\n",
    "* defining and generating the dataset X_train and X_val\n",
    "* visualize samples of the generated images\n",
    "* use keras to train a CNN with only one kernel (5x5 pixel)\n",
    "* visualize the weights of the leaned kernel and interpret if it is useful\n",
    "* repeat the last two steps to check if the learned kernel is always the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PDLAWRQ7iUB"
   },
   "outputs": [],
   "source": [
    "# load required libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n",
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oq0FNqcBpj23"
   },
   "source": [
    "### Defining functions to generate images \n",
    "\n",
    "Here we define the function to genere images with vertical and horizontal bars, the arguments of the functions are the size of the image and the nr of bars you want to have. The bars are at random positions in the image with a random length. The image is black and white, meaning we have only two values for the pixels, 0 for black and 255 for white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqVBlR8yAO9c"
   },
   "outputs": [],
   "source": [
    "#define function to generate image with shape (size, size, 1) with stripes\n",
    "def generate_image_with_bars(size, bar_nr, vertical = True):\n",
    "    img=np.zeros((size,size,1),dtype=\"uint8\")\n",
    "    for i in range(0,bar_nr):\n",
    "        x,y = np.random.randint(0,size,2)\n",
    "        l  = np.int(np.random.randint(y,size,1))\n",
    "        if (vertical):\n",
    "            img[y:l,x,0]=255\n",
    "        else:\n",
    "            img[x,y:l,0]=255\n",
    "    return img  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUmdGzQLdqzB"
   },
   "source": [
    "Let's have a look how the generated images. We choose a size of 50x50 pixels and set the nr of bars in the image to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "EccLz0FlXGuU",
    "outputId": "e19cd292-fab3-4dd4-bfc9-9808f93c7d20"
   },
   "outputs": [],
   "source": [
    "# have a look on two generated images\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "img=generate_image_with_bars(50,10, vertical=True)\n",
    "plt.imshow(img[:,:,0],cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "img=generate_image_with_bars(50,10, vertical=False)\n",
    "plt.imshow(img[:,:,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8gSwmyaevTk"
   },
   "source": [
    "### Make a train and validation dataset of images with vertical and horizontal images\n",
    "Now, let's make a train dataset *X_train* with 1000 images (500 images with vertical and 500 images with horizontal bars). We normalize the images values to be between 0 and 1 by dividing all values with 255. We create a secont dataste *X_val* with exactly the same properties to validate the training of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "63omuptEILKu",
    "outputId": "c1b63397-1539-4c26-c3b0-7b6e96778049"
   },
   "outputs": [],
   "source": [
    "pixel=50  # define height and width of images\n",
    "num_images_train = 1000 #Number of training examples (divisible by 2) \n",
    "num_images_val = 1000 #Number of training examples (divisible by 2) \n",
    "\n",
    "# generate training data with vertical edges\n",
    "X_train =np.zeros((num_images_train,pixel,pixel,1))\n",
    "for i in range(0, num_images_train//2):\n",
    "    X_train[i]=generate_image_with_bars(pixel,10)\n",
    "# ... with horizontal\n",
    "for i in range(num_images_train//2, num_images_train):\n",
    "    X_train[i]=generate_image_with_bars(pixel,10, vertical=False)\n",
    "\n",
    "# generate validation data \n",
    "X_val =np.zeros((num_images_train,pixel,pixel,1))\n",
    "for i in range(0, num_images_train//2): \n",
    "    X_val[i]=generate_image_with_bars(pixel,10)\n",
    "# ... with horizontal\n",
    "for i in range(num_images_train//2, num_images_train):\n",
    "    X_val[i]=generate_image_with_bars(pixel,10, vertical=False)\n",
    "\n",
    "\n",
    "# normalize the data to be between 0 and 1\n",
    "X_train=X_train/255\n",
    "X_val=X_val/255\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajNnUoYyi7IQ"
   },
   "source": [
    "Here we make the labels for the art lover, 0 means he likes the image and 1 means that he doesn't like it. We convert the labels into the one hot encoding becuase we want to use two outputs in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41-L5hM8S_ZP"
   },
   "outputs": [],
   "source": [
    "# create class labels\n",
    "y = np.array([[0],[1]])\n",
    "Y_train = np.repeat(y, num_images_train //2)\n",
    "Y_val = np.repeat(y, num_images_train //2)\n",
    "\n",
    "# one-hot-encoding\n",
    "Y_train=to_categorical(Y_train,2)\n",
    "Y_val=to_categorical(Y_val,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZpr0h-VvatF"
   },
   "source": [
    "## Defining the CNN to predict which images the art lover likes\n",
    "\n",
    "Here we define the kind of special architecture of the CNN: \n",
    "\n",
    ">we use only one kernel with a size of 5x5 pixels  \n",
    ">then we apply a linar activation function  \n",
    ">the maxpooling layer takes the maximum of the whole activation map to predict the probability (output layer with softmax) if the art lover will like the image\n",
    "\n",
    "as loss we use the categorical_crossentropy and we train for 20 epochs with a batchsize of 64 images per update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-0ZP6W_klrY"
   },
   "source": [
    "#### Listing 2.5 Edge lover CNN                                                                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Dfg1h2rUifd"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(1,(5,5),padding='same',input_shape=(pixel,pixel,1)))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# take the max over all values in the activation map\n",
    "model.add(MaxPooling2D(pool_size=(pixel,pixel)))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile model and initialize weights\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "r6eqV0TRU0_n",
    "outputId": "bf2c3cef-d031-4f80-a940-c1e2bf68fa24"
   },
   "outputs": [],
   "source": [
    "# let's summarize the CNN architectures along with the number of model weights\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Sc-BYd8kVCx0",
    "outputId": "d9f4fbd8-7c7d-4b4c-8798-f17cd0a47921"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "history=model.fit(X_train, Y_train,\n",
    "                  validation_data=(X_val,Y_val),\n",
    "                  batch_size=64, \n",
    "                  epochs=50,\n",
    "                  verbose=1,\n",
    "                  shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "fK_AAAoiQtlc",
    "outputId": "308015f1-814c-4ef8-ee3b-eda55f65a9fb"
   },
   "outputs": [],
   "source": [
    "# plot the development of the accuracy and loss during training\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,(1))\n",
    "plt.plot(history.history['accuracy'],linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.subplot(1,2,(2))\n",
    "plt.plot(history.history['loss'],linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOwR3Esbw8eN"
   },
   "source": [
    "### Visualize the learned kernel and experiment with the code\n",
    "\n",
    "You see that the CNN performs very good at this task (100% accuracy). We can check which pattern is recognized by the learned kernel and see if you think that this is helpful to distinguish between images with horizontal and vertical edges. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "colab_type": "code",
    "id": "pl1yuAddVRnE",
    "outputId": "c44c3489-23f1-4e49-82b2-cf1887904a67"
   },
   "outputs": [],
   "source": [
    "# get the leared weights and display them as image\n",
    "conv_filter=model.get_weights()[0]\n",
    "conv_filter.shape\n",
    "conv_filter=np.squeeze(conv_filter, axis=2)\n",
    "plt.imshow(conv_filter[:,:,0],\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look what this learned kernel actually does by trying it out on a picture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 as cv2\n",
    "# Loads an image\n",
    "src = cv2.imread('Pictures/schach.png', cv2.IMREAD_COLOR)\n",
    "# Check if image is loaded fine\n",
    "plt.figure(1)\n",
    "plt.imshow(src)\n",
    "ddepth = -1\n",
    "\n",
    "ind = 0\n",
    "\n",
    "kernel_size = 5\n",
    "kernel = np.ones((kernel_size, kernel_size), dtype=np.float32)\n",
    "print(kernel.shape)\n",
    "kernel = conv_filter.T\n",
    "kernel = kernel.squeeze(axis=0)\n",
    "#kernel /= (kernel_size * kernel_size)\n",
    "print(kernel)\n",
    "dst = cv2.filter2D(src, -1, kernel)\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.imshow(dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4gnnlAPp_Q2"
   },
   "source": [
    "#### Repeat the training and exerpiment with the kernelsize and activation function.<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />\n",
    "\n",
    "**Exercise:** Repeat the compiling and training for several times and check if the CNN always learns the same kernel.  \n",
    "You can experiment with the code and check what happens if you use another kernel size, activation function (relu instead of linear ) or pooling method AveragePooling instead of MaxPooling.  \n",
    "Try to make a prediction before doing the experiment.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRlCUwpVoy69"
   },
   "source": [
    "Credit: https://tensorchiefs.github.io/dl_book/\n",
    "\n",
    "with the two Notebooks taken:\n",
    "\n",
    "Banknotes: https://github.com/tensorchiefs/dl_book/blob/master/chapter_02/nb_ch02_01.ipynb\n",
    "\n",
    "Edgelover: https://github.com/tensorchiefs/dl_book/blob/master/chapter_02/nb_ch02_03.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nb_ch02_01.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
