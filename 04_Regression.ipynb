{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvQWEPI9CH7H"
   },
   "source": [
    "# Linear and Non-Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4EZzCr9CH7K"
   },
   "source": [
    "## Prerequisites:\n",
    "\n",
    "We recommend that you run this this notebook in the cloud on Google Colab or any other GPU accelerated Tensorflow Implementation. If you're not already doing so. It's the simplest way to get started. You can also [install TensorFlow locally](https://www.tensorflow.org/install/). But, again, simple is best (with caveats):\n",
    "\n",
    "[tf.keras](https://www.tensorflow.org/guide/keras) is the simplest way to build and train neural network models in TensorFlow. So, that's what we'll stick with in this tutorial, unless the models neccessitate a lower-level API.\n",
    "\n",
    "Note that there's [tf.keras](https://www.tensorflow.org/guide/keras) (comes with TensorFlow) and there's [Keras](https://keras.io/) (standalone). You should be using [tf.keras](https://www.tensorflow.org/guide/keras) because (1) it comes with TensorFlow so you don't need to install anything extra and (2) it comes with powerful TensorFlow-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1619,
     "status": "ok",
     "timestamp": 1601025605915,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "VZJzQXtUCH7L",
    "outputId": "83dcaace-d500-4645-cdff-65dc5e2665fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import IPython\n",
    "from six.moves import urllib\n",
    "%load_ext tensorboard\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Salary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('datasets/Salary_Data.csv', delimiter=',',skip_header=1)\n",
    "X = my_data[:,0]\n",
    "Y = my_data[:,1]\n",
    "#X = X/(np.max(X)) \n",
    "#Y = Y/(np.max(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,Y,'.')\n",
    "plt.xlabel(\"Working Years\")\n",
    "plt.ylabel(\"Salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression using Sklearn (Python package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "res = model.fit(X.reshape((len(X),1)), Y)\n",
    "predictions = model.predict(X.reshape((len(X),1)))\n",
    "plt.plot(X, predictions)\n",
    "plt.plot(X,Y,'.')\n",
    "plt.show()\n",
    "print(\"solpe = \", res.coef_[0],\"intercept = \",res.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression using a simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    if epoch < 15:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (15 - epoch))\n",
    "\n",
    "lr_scheduler_cbk = tf.keras.callbacks.LearningRateScheduler(lr_scheduler,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Create the Model once!\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(1,input_dim=1,activation='linear'))\n",
    "model.summary()\n",
    "\n",
    "opt = optimizers.SGD(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=opt, \n",
    "                  loss='mse')\n",
    "\n",
    "\n",
    "#Callbacks for easier Training\n",
    "#Train the Model with the training and validation Data\n",
    "# a=np.array(5000,dtype=\"float32\",ndmin=2)\n",
    "# b=np.array(2000,dtype=\"float32\",ndmin=1)\n",
    "# model.set_weights([a,b])\n",
    "\n",
    "history = model.fit(X, Y, epochs=EPOCHS,batch_size=X.shape[0],verbose=False,\n",
    "                   # callbacks=[lr_scheduler_cbk]\n",
    "                   )\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "plt.plot(history.epoch,history.history[\"loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = model.get_weights()\n",
    "print(f\"sk_slope={res.coef_[0]:.3f}, sk_intercept={res.intercept_:.3f}\")\n",
    "print(f\"tf_slope={a[0][0]:.3f}, tf_intercept={b[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,Y,'.')\n",
    "plt.plot(X,pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize the Neural Network\n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"40\" align=\"left\" />  \n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "    It seems that the linear regression for the Neural Network does not fit the datapoints. \n",
    "      * What are the reasons?\n",
    "      * How can we optimize it?  \n",
    "      * What are good Hyperparameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UydjrBCXCH7d"
   },
   "source": [
    "## Sinuswave Prediction with Feed Forward Neural Networks\n",
    "\n",
    "Let's start with using a fully-connected neural network to do predict the shape of datapoints, so called regression. The following image highlights the difference between regression and classification (see part 2). Given an observation as input, **regression** outputs a continuous value (e.g., exact temperature) and classificaiton outputs a class/category that the observation belongs to.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/u3TNL.png\" alt=\"classification_regression\" width=\"400\"/>\n",
    "\n",
    "Now, we load the dataset. Loading the dataset returns six NumPy arrays:\n",
    "\n",
    "* The `train points X` and `train points Y` arrays are the *training set*—the data the model uses to learn.\n",
    "* The model is tested against the *validation set*, the `validation X`, and `validation Y` arrays.\n",
    "* As we created the points based on a ground truth sinusoid, we also get two arrays `ground truth X` and `ground truth y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1601024332725,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "UuGc15ebCH7e"
   },
   "outputs": [],
   "source": [
    "# Example sinusoid dataset\n",
    "def createDataset_Sinusoid(xmin=-10., xmax=10, noise_std=.2):\n",
    "    num_data = 1000\n",
    "    # Create the noise training data\n",
    "    X_train = np.atleast_2d(np.linspace(xmin, xmax, num_data, dtype=np.float32)).T\n",
    "    y_train = np.sin(X_train) + np.atleast_2d(np.random.normal(0, noise_std, size=num_data).astype(np.float32)).T\n",
    "\n",
    "    # Create the ground truth\n",
    "    X_gt = X_train\n",
    "    y_gt = np.sin(X_gt)\n",
    "\n",
    "    # Create the validation data\n",
    "    X_val = np.atleast_2d(np.linspace(xmin-2, xmax+2, num_data, dtype=np.float32)).T\n",
    "    y_val = np.sin(X_val) + np.atleast_2d(np.random.normal(0, noise_std, size=num_data).astype(np.float32)).T\n",
    "    return [X_gt, y_gt,X_train, y_train, X_val, y_val]\n",
    "\n",
    "\n",
    "X_gt,y_gt,X_train,y_train,X_val,y_val = createDataset_Sinusoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1601024669169,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "fBq0I0oufQ81",
    "outputId": "453eb6ff-756e-459e-ebef-0a7c174cd437"
   },
   "outputs": [],
   "source": [
    "#Visualize the Data\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Visualization')\n",
    "ax.plot(X_train,y_train,'.',label='Training Data')\n",
    "ax.plot(X_gt,y_gt,label=\"Ground Truth\")\n",
    "ax.legend()\n",
    "\n",
    "# List for individual predictions\n",
    "preds_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czXjcPoXCH7n"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model. First we stack a few layers together using `keras.Sequential`. The number of Layers is very dependend on the tasks you want to perform.\n",
    "\n",
    "#### The Number of Hidden Layers\n",
    "\n",
    "| Num Hidden Layers       | Result        |  \n",
    "| ----------------------- |:------------- | \n",
    "| None               | Only capable of representing linear separable functions or decisions.      |\n",
    "| 1          | Can approximate any function that contains a continuous mapping from one finite space to another.      |\n",
    "| 2          | Can represent an arbitrary decision boundary to arbitrary accuracy <br> with rational activation functions and can approximate any smooth mapping to any accuracy.     |\n",
    "| >2          | Additional layers can learn complex representations (sort of automatic feature engineering) for layer layers.      |\n",
    "\n",
    "#### The Number of Neurons in the Hidden Layers\n",
    "\n",
    "There are many rule-of-thumb methods for determining an acceptable number of neurons to use in the hidden layers, such as the following:\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "These three rules provide a starting point for you to consider the number of neurons. There is also some research about this topic: https://www.hindawi.com/journals/mpe/2013/425740/\n",
    "\n",
    "\n",
    "Next we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
    "\n",
    "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
    "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
    "* *Metrics* - used to monitor the training and testing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "**Exercise:**\n",
    "Let's build a network with multiple hidden layer, and use mean squared error (MSE) as the loss function (most common one for regression problems). Your networks structure `could` look similar to this:\n",
    "\n",
    "<img src=\"Pictures/Reg_Model.PNG\" align=\"middle\" />\n",
    "\n",
    "The last layer of the Network should also be a dense layer and the number of neurons depends on the task you want to do. In this example we want to predict y coordinates based on x coordinates, so the output should only have 1 neuron.\n",
    "\n",
    "**Hint**: If you run into trouble, what is missing? See the error output.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Click here for one possible solution</b></summary>\n",
    "    \n",
    "```python\n",
    "def build_model():\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    ############ YOUR CODE HERE ############\n",
    "    model = keras.Sequential()\n",
    "    #The first layer is called an Input layer and has the shape of our input data\n",
    "    model.add(Input(shape=(X_train.shape[-1]), name='input'))\n",
    "    #add as much Dense layers as you want \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    # Output Layer with 1 Neuron as Output\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    return model\n",
    "\n",
    "# If you run into an error compile is missing\n",
    "model.compile(loss=\"mse\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1601027756350,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "fODcoqWfCH7p"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    ############ YOUR CODE HERE ############\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Output Layer with 1 Neuron as Output\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1601027758661,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "QMkrY4X3mPQs",
    "outputId": "d848a4e7-0aa8-4eb6-abd1-28445d799cf8"
   },
   "outputs": [],
   "source": [
    "# Build the model and show the layers\n",
    "model = build_model()\n",
    "#opti = tf.keras.optimizers.Adam()\n",
    "#opti = tf.keras.optimizers.SGD()\n",
    "#opti = tf.keras.optimizers.Adamax()\n",
    "\n",
    "model.compile(optimizer=opti,loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OjDiTB8WCH7w"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Training the neural network model requires the following steps:\n",
    "\n",
    "1. Feed the training data to the model—in this example, the `train_features` and `train_labels` arrays.\n",
    "2. The model learns to associate features and labels.\n",
    "3. We ask the model to make predictions about a test set—in this example, the `test_features` array. We verify that the predictions match the labels from the `test_labels` array. \n",
    "\n",
    "To start training,  call the `model.fit` method—the model is \"fit\" to the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and logging the network\n",
    "\n",
    "There are a few very importent metrics we need to look at while training the Network. As described in the earlier notebooks, the loss is very important. So while training we want to display it so that we can make sure our network learns something and isnt doing bad stuff. \n",
    "\n",
    "This is done by the model.fit function. Here we specify the train and the validation data and while training we get a nice output that shows us all the important values like validation loss and the training loss. \n",
    "\n",
    "On the Other hand we want to look at this data after we trained the model. We can use the output of the model.fit function after training to view the raw data ourself or we use so called tensorboard. For using that we need to specify a log file and location and use the tensorboard as callback for the model.fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12729,
     "status": "ok",
     "timestamp": 1601027773541,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "B2WEm1_aCH7y",
    "outputId": "a742531a-a8a2-4ab2-bef3-5fd8e7795cda",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Callbacks for easier Training\n",
    "import datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "# tensorboard lets us view at the training data afterwards! \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "#Train the Model with the training and validation Data\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS,verbose=0,\n",
    "                    callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "# Manual plotting the training data after the training is done via the history dataframe\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Tensorboard for evaluation\n",
    "Tensorboard is a nice tool within tensorflow that gives you some model insights. Within the tensorboard you can investigate:\n",
    " * Graphs\n",
    " * Layers\n",
    " * Training and validation\n",
    " * etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs --port 6006 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MIPcQ3QCH76"
   },
   "source": [
    "Now, let's plot the loss function measure on the training and validation sets. The validation set is used to prevent overfitting ([learn more about it here](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)). However, because our network is small, the training convergence without noticeably overfitting the data as the plot shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1601027797811,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "6pTN3z69CH77",
    "outputId": "e0c50fc9-ca84-430f-9f91-5a1337b2f1fd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "############ YOUR CODE HERE ############\n",
    "# If you want to plot more metrics e.g. \"accuracy\"\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHTdAj5unABE"
   },
   "source": [
    "We trained the model to predict the Y Value based on the X Value. Now we can test it by predicting with some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1601027800287,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "laPO2jXVCH8E"
   },
   "outputs": [],
   "source": [
    "preds_list.append([model.predict(X_val),f\"{opti.__class__.__name__.lower()}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rb3cfHMICH8D"
   },
   "source": [
    "Next, compare how the model performs on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1601027802066,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "GChCh5zdh-Gl",
    "outputId": "bd7d0478-fe96-406a-e54d-4e7d41d0e68d"
   },
   "outputs": [],
   "source": [
    "#Visualize the Data\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(f'Visualization after {EPOCHS} Epochs Training')\n",
    "\n",
    "# First we plot the Data we predicted on\n",
    "ax.plot(X_val,y_val,'.',label='Test Data')\n",
    "# The Ground Truth\n",
    "ax.plot(X_gt,y_gt,label=\"Ground Truth\")\n",
    "# We predicted the y Value based on the X_val value\n",
    "\n",
    "for p,l in preds_list:\n",
    "    ax.plot(X_val,p,label=f\"prediction_{l}\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuweaoFwp4sH"
   },
   "source": [
    "### Improve the model\n",
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "**Exercise:** Now you have some time to improve the prediction. If you have any question just ask.\n",
    "Please store your predictions in a list to evaluate if your model improved.\n",
    "\n",
    "<details>\n",
    "    <summary><b> Click here for some tips to improve the model:</b></summary>\n",
    "\n",
    "  1. Try adding more Dense Layers\n",
    "  2. Try adding Dropout layers `tf.keras.layers.Dropout(rate=0.0..0.5)`\n",
    "  3. Increase Training Time\n",
    "  4. Change the optimizer (SGD,ADAM,ADAMAX)\n",
    "  5. Check your predictions after new training\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " Workshop tutorial_deep_learning_basics.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb",
     "timestamp": 1601022100465
    }
   ],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
