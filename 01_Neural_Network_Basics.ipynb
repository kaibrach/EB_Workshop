{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Introduction\n",
    "\n",
    "This notebook shall give you an introduction to the basics of neural networks. We will discuss:\n",
    "1. Deep Learning Basics\n",
    "2. Layer structure of neural networks\n",
    "3. Loss functions\n",
    "4. Optimizers\n",
    "5. Activation functions\n",
    "\n",
    "Further we will introduce [Tensorflow ](https://www.tensorflow.org/) as Deep Learning Framework. TensorFlow is an end-to-end open source platform for machine learning.\n",
    "\n",
    "Some helpful **Cheat-Sheets** for **Python/Numpy/Tensorflow**: \n",
    "* [Tensorflow_Keras_CheatSheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras.layers import Input,Dense,Activation,Conv2D\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    "This is just a helper function for plotting Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the auxiliary function forming the diagram\n",
    "def make_plot(x, f, df=None, name='Enter Name',f_name=None,df_name=None):\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(name, fontsize=20, fontweight='bold')\n",
    "    plt.xlabel('z', fontsize=15)\n",
    "    plt.ylabel('Activation function value', fontsize=15)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    if f_name is None:\n",
    "        plt.plot(x, f, label=\"f (z)\")\n",
    "    else:\n",
    "        plt.plot(x, f, label=f_name)\n",
    "    if df is not None:\n",
    "        if df_name is None:    \n",
    "            plt.plot(x, df, label=\"f '(z)\")\n",
    "        else:\n",
    "            plt.plot(x, df, label=df_name)\n",
    "        \n",
    "    plt.legend(loc=4, prop={'size': 15}, frameon=True,shadow=True, facecolor=\"white\", edgecolor=\"black\")\n",
    "    #plt.savefig(f'..\\\\doc\\\\pics\\\\activation_functions\\\\{name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Deep Learning Basics\n",
    "\n",
    "This tutorial accompanies the [lecture on Deep Learning Basics](https://www.youtube.com/watch?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&v=O5xeyoRL95U) given as part of [MIT Deep Learning](https://deeplearning.mit.edu).\n",
    "\n",
    "In this tutorial, we mention seven important types/concepts/approaches in deep learning, introducing the first 2 and providing pointers to tutorials on the others. Here is a visual representation of the seven:\n",
    "\n",
    "![Deep learning concepts](https://i.imgur.com/EAl47rp.png)\n",
    "\n",
    "At a high-level, neural networks are either encoders, decoders, or a combination of both. Encoders find patterns in raw data to form compact, useful representations. Decoders generate new data or high-resolution useful infomation from those representations. As the lecture describes, deep learning discovers ways to **represent** the world so that we can reason about it. The rest is clever methods that help use deal effectively with visual information, language, sound (#1-6) and even act in a world based on this information and occasional rewards (#7).\n",
    "\n",
    "1. **Feed Forward Neural Networks (FFNNs)** - classification and regression based on features. See [Part 1](#Part-1:-Boston-Housing-Price-Prediction-with-Feed-Forward-Neural-Networks) of this tutorial for an example.\n",
    "2. **Convolutional Neural Networks (CNNs)** - image classification, object detection, video action recognition, etc. See [Part 2](#Part-2:-Classification-of-MNIST-Dreams-with-Convolution-Neural-Networks) of this tutorial for an example.\n",
    "3. **Recurrent Neural Networks (RNNs)** - language modeling, speech recognition/generation, etc. See [this TF tutorial on text generation](https://www.tensorflow.org/tutorials/sequences/text_generation) for an example.\n",
    "4. **Encoder Decoder Architectures** - semantic segmentation, machine translation, etc. See [our tutorial on semantic segmentation](https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb) for an example.\n",
    "5. **Autoencoder** - unsupervised embeddings, denoising, etc.\n",
    "6. **Generative Adversarial Networks (GANs)** - unsupervised generation of realistic images, etc. See [this TF tutorial on DCGANs](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb) for an example.\n",
    "7. **Deep Reinforcement Learning** - game playing, robotics in simulation, self-play, neural arhitecture search, etc. We'll be releasing notebooks on this soon and will link them here.\n",
    "\n",
    "There are selective omissions and simplifications throughout these tutorials, hopefully without losing the essence of the underlying ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Layers\n",
    "\n",
    "A Neural Network consists of various different layers and can utilize different architectures to conquer the problems you have. The main principles all networks share are the Input and Output layers. Those have to be fitted specifically for your problems.\n",
    "\n",
    "<img src=\"Pictures/small_fcn.PNG\" width=\"500px\" >\n",
    "\n",
    "Now we want to build the network model as discribed in the picture using Tensorflow.\n",
    "Therefore we create two different models using `functional` and `sequential` api\n",
    "\n",
    "**1. Functional Api:** The functional API allows you to create models that have a lot more flexibility as you can easily define models where layers connect to more than just the previous and next layers. In fact, you can connect layers to (literally) any other layer. As a result, creating complex networks such as siamese networks and residual networks become possible.\n",
    "\n",
    "**2. Sequential Api:**\n",
    "The sequential API allows you to create models layer-by-layer for most problems. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras.\n",
    "K.clear_session()\n",
    "\n",
    "# With the keras functional api:\n",
    "x = Input(shape=(3,))\n",
    "z = Dense(4)(x)\n",
    "g = Activation(activation=\"relu\")(z)\n",
    "y = Dense(2)(g)\n",
    "model = Model(x, y)\n",
    "model.summary()\n",
    "\n",
    "print()\n",
    "\n",
    "# With the keras sequential api:\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(3,)))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dense(2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "The Input layer is always the first layer in the network. It makes sure to fit the input data (x-values,pictures, vectors) into Tensors, a shape that the network can understand. \n",
    "The basic implementation in Tensorflow/Keras looks like this:\n",
    "\n",
    "``` python\n",
    "x = Input(shape=(32,))\n",
    "x = Input(shape=(28,28,1))\n",
    "x = Input(shape=(512,512,3))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer\n",
    "A dense layer consists of multiple neurons that are connected with all the neurons in the previous layer. In Tensorflow/Keras you can implement it using the Dense() Function, where you also can specify how many neurons this layer should have and what the activation should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras.\n",
    "K.clear_session()\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.uniform(low=-1,high=1,size=(3,1)),dtype=tf.float32)\n",
    "# Create a dense layer with linear activation and calulate the outputs of random inputs\n",
    "layer = Dense(3, activation='linear')\n",
    "y = layer(x)\n",
    "\n",
    "print(f\"inputs:\\n {x.numpy().flatten()}\")\n",
    "print(f\"weights:\\n {layer.weights[0].numpy().flatten()}\")\n",
    "print(f\"y:\\n {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "           1. Calulate the output by hand! \n",
    "           \n",
    "**Hint**\n",
    "`layer.weights` returns weights and bias\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><b>Click here for one possible solution</b></summary>\n",
    "    \n",
    "```python\n",
    "w = layer.weights[0] # Weights\n",
    "b = layer.weights[1] # Bias\n",
    "y_by_hand = x*w+b\n",
    "\n",
    "print(f\"y_by_hand:\\n {y_by_hand}\") # dot(input, weights) + bias\n",
    "\n",
    "(y.numpy() == y_by_hand.numpy()).all()\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ YOUR CODE HERE ############\n",
    "y_by_hand = \n",
    "\n",
    "print(f\"y_by_hand:\\n {y_by_hand}\") # dot(input, weights) + bias\n",
    "\n",
    "# Print True if y and y_by_hand is equal\n",
    "(y.numpy() == y_by_hand.numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras.\n",
    "tfk.backend.clear_session()\n",
    "\n",
    "# Dense\n",
    "x = Input(shape=(32,))\n",
    "y = Dense(16, activation='relu')(x)\n",
    "y = Dense(32, activation='relu')(y)\n",
    "y = Dense(64, activation='linear')(y)\n",
    "model = Model(x, y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "           1. Can you calculate the Params yourself? \n",
    "           2. How do they compute? \n",
    "<details>\n",
    "<summary><b>Click here for one possible solution</b></summary>\n",
    "\n",
    "dense_params   = 32*16+16 : input*weights+bias <br>\n",
    "dense_1_params = 16*32+32 : input*weights+bias <br>\n",
    "dense_2_params = 32*64+64 : input*weights+bias\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ YOUR CODE HERE ############\n",
    "dense_params = 0\n",
    "dense_1_params = 0\n",
    "dense_2_params = 0\n",
    "\n",
    "print(f\"Layer dense: {dense_params}\")\n",
    "print(f\"Layer dense_1: {dense_1_params}\")\n",
    "print(f\"Layer dense_2: {dense_2_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers\n",
    "\n",
    "\n",
    "The basic structure is a little bit different now as we have a two dimensional picture. \n",
    "We use so called **Convolutions** to compress (and shrink) the information in the picture. They use filters to 'scan' the picture and detect edges or important parts of the image.\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/keras-conv2d/keras_conv2d_padding.gif\" width=\"500px\" >\n",
    "\n",
    "If the information is dense enough after those convolutions we can flatten the remaining picture (the so called feature map) and use a normal Dense layer to make the prediction\n",
    "\n",
    "\n",
    "<img src=\"https://missinglink.ai/wp-content/uploads/2019/03/Frame-16.1.png\" width=\"700px\" >\n",
    " \n",
    " Those convolution layers have two effects, they extract the information from the picture and they shrink the picture if you dont apply some sort of frame around the picture (see first gif). This shrinking actually is a good thing as we want the picture to get smaller while we compress the information.\n",
    " \n",
    "in Tensorflow/Keras the Convolution is called `Conv2D` (for 2 dimensional convolutions). With this function you need to specify the number of filters you want to have ( each filter creates one feature map,e.g one dimension) and how big the filter is. \n",
    "\n",
    "`Calulation of params = (kernel_height * kernel_width * input_image_channels + 1) * number_of_filters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras.\n",
    "tfk.backend.clear_session()\n",
    "\n",
    "# this is a logistic regression in Keras\n",
    "x = Input(shape=(28,28,1))\n",
    "y = Conv2D(filters=3,kernel_size=(3,3), activation='softmax')(x)\n",
    "model = Model(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "Pooling layers help reducing the computational effort in the network by 'shrinking' the input. This is done by grouping small areas by their max/min or mean value to preserve the most important information while reducing the input size!\n",
    "<img src=\"Pictures/pooling.png\" width=\"700px\" > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras.\n",
    "tfk.backend.clear_session()\n",
    "\n",
    "# this is a logistic regression in Keras\n",
    "x = Input(shape=(28,28,1))\n",
    "y = Conv2D(filters=3,kernel_size=(3,3))(x)\n",
    "y = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(y)\n",
    "model = Model(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1\n",
    "H=4\n",
    "W=4\n",
    "C=1\n",
    "# Create a Tensor(Matrix) with random numbers and the dimension 4x4 \n",
    "tensor = tf.convert_to_tensor(np.random.choice(np.arange(1,20),[N,H,W,C]),dtype=tf.float32)\n",
    "x = tensor.numpy()[0,:,:,:].transpose(2,0,1)\n",
    "print('Tensor bevor max_pool2d operation:')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do MaxPooling on the tensor\n",
    "max_pool = tf.nn.max_pool2d(tensor,ksize=[1,2,2,1],\n",
    "                            strides=[1,2,2,1],padding='SAME')\n",
    "print(f'Tensor after max_pool2d operation:')\n",
    "print(max_pool.numpy()[0,:,:,:].transpose(2,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "               1. Try the same operation with the `average_pool2d` operation\n",
    "               2. What do you think, which pooling operation is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ YOUR CODE HERE ############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Pooling vs. Max Pooling\n",
    "Compare average pooling and max pooling on a custom binary image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from skimage.measure import block_reduce\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Just play with the kernel-sizes\n",
    "H = 9\n",
    "W = 9\n",
    "\n",
    "# Read two images\n",
    "imgs = [imread('Pictures/black_x.png'), imread('Pictures/white_x.png')]\n",
    "\n",
    "for img in imgs:\n",
    "\n",
    "    max_pool = tf.nn.max_pool2d(np.expand_dims(img,0),ksize=[1,H,W,1],\n",
    "                                 strides=[1,H,W,1],padding='VALID').numpy()[0,:]\n",
    "    avg_pool = tf.nn.avg_pool2d(np.expand_dims(img,0),ksize=[1,H,W,1],\n",
    "                                 strides=[1,H,W,1],padding='VALID').numpy()[0,:]\n",
    "    \n",
    "#     avg_pool=block_reduce(img, block_size=(H,W,1), func=np.mean)\n",
    "#     max_pool=block_reduce(img, block_size=(H,W,1), func=np.max)\n",
    "    min_pool=block_reduce(img, block_size=(H,W,1), func=np.min) # Not included in tensorflow :-(\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(141)\n",
    "    imgplot = plt.imshow(img,cmap=plt.cm.binary)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.subplot(142)\n",
    "    imgplot3 = plt.imshow(min_pool, cmap=plt.cm.binary)\n",
    "    plt.title('Min pooling')\n",
    "\n",
    "    plt.subplot(143)\n",
    "    imgplot1 = plt.imshow(avg_pool, cmap=plt.cm.binary)\n",
    "    plt.title('Avg pooling')\n",
    "    \n",
    "    plt.subplot(144)\n",
    "    imgplot1 = plt.imshow(max_pool, cmap=plt.cm.binary)\n",
    "    plt.title('Max pooling')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Min pooling gives better result for images with white background and black object\n",
    "* Avg pooling gives same results regardless if background is black or white\n",
    "* Max pooling gives better result for the images with black background and white object (Ex: MNIST dataset)\n",
    "\n",
    "When classifying the `MNIST` digits dataset using CNN, max pooling is used because the background in these images is made black to reduce the computation cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout \n",
    "\n",
    "If you have a training set for your neural network that has very few samples, your network tends to overfit this data. That means that it is likely to learn features that only the training data includes resulting in bad prediction for other data that was not included in the dataset. \n",
    "\n",
    "Dropout is used to prevent exactly that by dropping out different neurons in each layer after every training step. This forces the network to take different paths even if the input data would be the same resulting in a more robust training and reducing the risk of overfitting! \n",
    "\n",
    "<img src=\"pictures/dropout.png\" width=\"500\" align=\"middle\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine this Tensor (Vector) as one Layer with weights (random numbers) in your Network\n",
    "tensor = tf.keras.backend.random_normal(shape=[10,1],mean=0.,stddev=1.)\n",
    "\n",
    "# Specify the dropout rate (Ratio)\n",
    "# Inputs elements are randomly set to zero (and the other elements are rescaled). \n",
    "# This encourages each node to be independently useful, as it cannot rely on the output of other nodes.\n",
    "dropout_rate = 0.3\n",
    "drop_out_tensor = tf.nn.dropout(tensor,rate=dropout_rate)\n",
    "\n",
    "\n",
    "print(\"Input Tensor:\\n\",tensor.numpy())\n",
    "print(\"----------------\")\n",
    "print(\"Dropout Tensor:\\n\",drop_out_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to notice is, that the sum of the weights has to stay the same, thus the remaining (non zero) weights are rescalled with a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original sum of weights: {tensor.numpy().sum()}')\n",
    "scale = 1/(1-dropout_rate)\n",
    "\n",
    "print(f'Dropout Tensor sum: {np.sum(drop_out_tensor[drop_out_tensor!=0].numpy())}')\n",
    "print(f'Original Tensor scaled: {np.sum((tensor*scale)[drop_out_tensor!=0].numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [imread('Pictures/black_x.png'), imread('Pictures/white_x.png')]\n",
    "\n",
    "dropout_rate = 0.3\n",
    "for img in imgs:\n",
    "\n",
    "    do_img = tf.nn.dropout(img,rate=dropout_rate).numpy()\n",
    "    do_img = np.uint8((255*do_img)/do_img.max())\n",
    "    \n",
    "#     avg_pool=block_reduce(img, block_size=(H,W,1), func=np.mean)\n",
    "#     max_pool=block_reduce(img, block_size=(H,W,1), func=np.max)\n",
    "    min_pool=block_reduce(img, block_size=(H,W,1), func=np.min) # Not included in tensorflow :-(\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(141)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.subplot(142)\n",
    "    imgplot3 = plt.imshow(do_img)\n",
    "    plt.title('Dropout')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "As part of the optimization algorithm, the error for the current state of the model must be estimated repeatedly. This requires the choice of an error function, conventionally called a loss function, that can be used to estimate the loss of the model so that the weights can be updated to reduce the loss on the next evaluation.\n",
    "\n",
    "Neural network models learn a mapping from inputs to outputs from examples and the choice of loss function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/404/1*TCE9Kui4fbyZl5u3ARRBJw.png\" align=\"center\" style=\"width:30%\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 1.]]\n",
    "y_pred = [[0., 1.5], [0., 1.2]]\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Crossentropy\n",
    "Binary classification loss function comes into play when solving a problem involving just **two classes**. \n",
    "For example, when predicting if a banknote is real or fake. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/548/1*PDtIfRHpMfbbXbhj26I-OA.png\" align=\"center\" style=\"width:50%\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 0.], [0., 0.]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "bce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Crossentropy\n",
    "Categorical Crossentropy is used when you have to classify between **multiple classes** (non binary). Here you most likely have your true_values as `one-hot-encoded vectors`, where the true class has a 1 and all the other classes have a 0. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*LIil7qwrVehQ8RxpwNXeUw.png\" align=\"center\" style=\"width:50%\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0, 1, 0], [0, 0, 1]]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "After defining the loss function we need to optimize it, so that it gets minimal. This is what the optimizer does. It takes the loss and optimizes it to a mimimum (carefull: this may also be a local minimum not a global one). Therefore we use gradient decent. Gradients in neural networks refer to vectors whose magnitude is the partial derivative of the function f(x) and is directed towards the greatest rate of increase of that function.\n",
    "\n",
    "<img src=\"Pictures/optimizers.png\" width=\"700px\" > \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1):\n",
    "    return x1**2.0 -4*x1\n",
    "\n",
    "def f2(x1,x2):\n",
    "    return x1**2.0 -x1*5 + x2**2.0\n",
    "\n",
    "x = np.linspace(-5, 5, 300, dtype=np.float32)\n",
    "\n",
    "\n",
    "plt.title('Function x**2 -2x')\n",
    "plt.plot(x,f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#starting value (randomly selected)\n",
    "x1 = tf.Variable(15.0)\n",
    "x2 = tf.Variable(5.0)\n",
    "\n",
    "# # Using the build in optimizer \n",
    "# def f_opt():\n",
    "#     return x1**2.0 -x1*5 + x2**2.0\n",
    "\n",
    "# opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "# loss_fn = f_opt\n",
    "# var_list_fn = [x1,x2]\n",
    "# for i in range(0,100):\n",
    "#     if i%10 == 0:\n",
    "#         print(f\"y={loss_fn().numpy()} x1={x1.numpy()} x2={x2.numpy()}\")\n",
    "#     opt.minimize(loss_fn, var_list_fn)\n",
    "\n",
    "\n",
    "# # Apply gradient to optimizer\n",
    "# for i in range(0,100):\n",
    "#     with tf.GradientTape() as t:\n",
    "#         y = f2(x1,x2)\n",
    "#     g = t.gradient(y,[x1,x2])\n",
    "#     if i%10 == 0:\n",
    "#     g_and_v = zip([a for a in g],[x1,x2])\n",
    "#     opt.apply_gradients(g_and_v)\n",
    "\n",
    "\n",
    "# Computing the gradient by hand\n",
    "l = []\n",
    "learning_rate = 0.01\n",
    "for i in range(0,500):\n",
    "    #Calculating the gradient at each step!\n",
    "    with tf.GradientTape() as t:\n",
    "        y = f(x1)\n",
    "    # calculated gradient\n",
    "    g = t.gradient(y,[x1])\n",
    "    # new x value calculated using the gradient and the learning rate\n",
    "    x1.assign(x1-learning_rate*g[0].numpy())\n",
    "\n",
    "    l.append(x1.numpy())\n",
    "\n",
    "\n",
    "#Plotting the result    \n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Loss curve')\n",
    "plt.plot(l)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('calculated minimum')\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x1.numpy(),f(x1.numpy()),'o')\n",
    "plt.legend(['function','minimum'])\n",
    "plt.tight_layout()\n",
    "print(f\"Zeropoint for x**2 -2x: x1={x1.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
    "\n",
    "**Small Exercise:** Play with the learning rate and see what happens if you make it smaller or larger, what happens if the learning rate is > 1 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction. Activation functions also help normalize the output of each neuron to a range between 1 and 0 or between -1 and 1.\n",
    "\n",
    "An additional aspect of activation functions is that they must be computationally efficient because they are calculated across thousands or even millions of neurons for each data sample. Modern neural networks use a technique called backpropagation to train the model, which places an increased computational strain on the activation function, and its derivative function.\n",
    "\n",
    "## Why we use Activation Functions\n",
    "\n",
    "Activation functions are one of the key elements of the neural network. Without them, our neural network would become a combination of linear functions, so it would be just a linear function itself. \n",
    "\n",
    "The non-linearity element allows for greater flexibility and creation of complex functions during the learning process. The activation function also has a significant impact on the speed of learning, which is one of the main criteria for their selection. Currently, the most popular one for hidden layers is probably `ReLU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "$${\\displaystyle f(x)={x}}\n",
    "\\hspace{1cm}\n",
    "{\\displaystyle f'(x) = 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, 0.01)\n",
    "f = z\n",
    "df = np.ones_like(z)\n",
    "make_plot(z, f, df, \"Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "$${\\displaystyle f(x)={\\frac {1}{1+e^{-x}}}}\n",
    "\\hspace{1cm}\n",
    "{\\displaystyle f'(x) = f(x)\\cdot(1-f(x))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, 0.01)   \n",
    "f = 1/(1 + np.exp(-z))\n",
    "df = f*(1 - f)\n",
    "make_plot(z, f, df, \"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "$${\\displaystyle f(x) = \\tanh(x)={\\frac {\\sinh(x)}{\\cosh (x)}}={\\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}}\n",
    "\\hspace{1cm}\n",
    "{\\displaystyle f'(x) = 1 - f(x)^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, 0.01)\n",
    "f = np.tanh(z)\n",
    "df = 1 - f*f\n",
    "make_plot(z, f, df, \"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU\n",
    "\n",
    "$${\\displaystyle f(x) = \\begin{cases}\n",
    "x & \\text{ if } x>0 \\\\ \n",
    "\\alpha(e^z-1) & \\text{ if } x\\leq x \n",
    "\\end{cases}}\n",
    "\\hspace{1cm}\n",
    "{\\displaystyle f'(x) = \\begin{cases}\n",
    "1 & \\text{ if } x>0 \\\\ \n",
    "\\alpha(e^z) & \\text{ if } x< x \n",
    "\\end{cases}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=1.\n",
    "z = np.arange(-10, 10, 0.01)\n",
    "f = np.where(z > 0, z, alpha*(np.exp(z) - 1))\n",
    "df = np.where(z > 0, 1, alpha*(np.exp(z)))\n",
    "make_plot(z, f, df, \"ELU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "$${\\displaystyle f(x) = \\begin{cases}\n",
    "0 & \\text{ if } x<0 \\\\ \n",
    "x & \\text{ if } x\\geq x \n",
    "\\end{cases}}\n",
    "\\hspace{1cm}\n",
    "{\\displaystyle f'(x) = \\begin{cases}\n",
    "0 & \\text{ if } x<0 \\\\ \n",
    "1 & \\text{ if } x\\geq x \n",
    "\\end{cases}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, 0.01)\n",
    "f = z * (z > 0)\n",
    "df = 1. * (z > 0)\n",
    "make_plot(z, f, df, \"ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "$${\\displaystyle f(x) = \\begin{cases}\n",
    "0.01 \\cdot x & \\text{ if } x<0 \\\\ \n",
    "x & \\text{ if } x\\geq x \n",
    "\\end{cases}}\n",
    "\\hspace{1cm}\n",
    "{\\displaystyle f'(x) = \\begin{cases}\n",
    "0.01 & \\text{ if } x<0 \\\\ \n",
    "1 & \\text{ if } x\\geq x \n",
    "\\end{cases}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, 0.01)\n",
    "f = np.where(z > 0, z, z * 0.01)\n",
    "df = np.where(z > 0, 1, 0.01)\n",
    "make_plot(z, f, df, \"Leaky_ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "\n",
    "$${\\displaystyle f(x) = ln(1+e^{x})}\n",
    "\\hspace{1cm}\n",
    "{\\displaystyle f'(x)={\\frac {e^x}{1+e^{x}}}={\\frac {1}{1+e^{-x}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, 0.01)\n",
    "f = np.log(1 + np.exp(z))\n",
    "df = 1/(1 + np.exp(-z)) # = Sigmoid\n",
    "make_plot(z, f, df, \"Softplus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10, 10, 0.01)\n",
    "f = np.log(1 + np.exp(z))\n",
    "df = z * (z > 0)\n",
    "make_plot(z, f, df, \"Softplus vs Relu\",f_name='Softplus',df_name='Relu')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
