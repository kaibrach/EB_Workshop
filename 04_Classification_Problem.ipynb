{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvQWEPI9CH7H"
   },
   "source": [
    "# Classification using FCN and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1619,
     "status": "ok",
     "timestamp": 1601025605915,
     "user": {
      "displayName": "Philipp Schmieder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilNjhUXq83sj1BsL2I_E5qGbVh7zCWNq5dOjxMMA=s64",
      "userId": "17931796020654859734"
     },
     "user_tz": -120
    },
    "id": "VZJzQXtUCH7L",
    "outputId": "83dcaace-d500-4645-cdff-65dc5e2665fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import IPython\n",
    "from six.moves import urllib\n",
    "%reload_ext tensorboard\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtYF5pJXCH8R"
   },
   "source": [
    "The MNIST dataset contains 70,000 grayscale images of handwritten digits at a resolution of 28 by 28 pixels. The task is to take one of these images as input and predict the most likely digit contained in the image (along with a relative confidence in this prediction):\n",
    "<img src=\"https://miro.medium.com/max/530/1*VAjYygFUinnygIx9eVCrQQ.png\" width=\"500px\" height=\"400px\">\n",
    "<img src=\"https://i.imgur.com/ITrm9x4.png\" width=\"500px\">\n",
    "\n",
    "Now, we load the dataset. The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255. The *labels* are an array of integers, ranging from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntuOwYNcCH8S"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape images to specify that it's a single channel\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned a fully connected neural network uses only a 1D Vector as Input data. So what we have to di is flatten out the existing pictures to a linear Data of 784 individual points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "image_vector_size  = train_images.shape[1]*train_images.shape[2]\n",
    "print(image_vector_size)\n",
    "X_train = train_images.reshape((-1, image_vector_size))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOkWmsCmCH8h"
   },
   "source": [
    "Display the first 5 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dkGPbCqCH8i",
    "outputId": "14e535ea-08e9-4806-af10-6ec701489171"
   },
   "outputs": [],
   "source": [
    "# Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> <b>Click here for one possible solution</b></summary>\n",
    "    \n",
    "```python\n",
    "plt.figure(figsize=(10,2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Classification of MNIST using Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model that looks like this: \n",
    "    \n",
    "<img src=\"Pictures/Class_FC.png\" width=\"700px\" > \n",
    "\n",
    "the activation on each hidden layer should be 'relu' and on the last layer 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1]), name='input'))\n",
    "#YOUR TURN\n",
    "# First hidden dense layer with 256 logits\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# First hidden dense layer with 256 logits\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Third hidden dense layer with 65 logits\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Click here for one possible solution</b></summary>\n",
    "    \n",
    "```python\n",
    "# First hidden dense layer with 256 logits\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# First hidden dense layer with 256 logits\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Third hidden dense layer with 65 logits\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 242,762\n",
      "Trainable params: 242,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   1/1688 [..............................] - ETA: 0s - loss: 101.6791 - accuracy: 0.0938WARNING:tensorflow:From C:\\Users\\phsc270152\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1688 [..............................] - ETA: 2:55 - loss: 78.7936 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.1980s). Check your callbacks.\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 1.0742 - accuracy: 0.8751 - val_loss: 0.2610 - val_accuracy: 0.9322\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 11s 7ms/step - loss: 0.2342 - accuracy: 0.9379 - val_loss: 0.2062 - val_accuracy: 0.9480\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1849 - accuracy: 0.9498 - val_loss: 0.1504 - val_accuracy: 0.9602\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 11s 6ms/step - loss: 0.1531 - accuracy: 0.9585 - val_loss: 0.2180 - val_accuracy: 0.9447\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1383 - accuracy: 0.9621 - val_loss: 0.1241 - val_accuracy: 0.9653\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1082 - accuracy: 0.9696 - val_loss: 0.1234 - val_accuracy: 0.9645\n",
      "Epoch 7/15\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0928 - accuracy: 0.9743 - val_loss: 0.1072 - val_accuracy: 0.9740\n",
      "Epoch 8/15\n",
      "1688/1688 [==============================] - 11s 7ms/step - loss: 0.0778 - accuracy: 0.9772 - val_loss: 0.1143 - val_accuracy: 0.9683\n",
      "Epoch 9/15\n",
      "1688/1688 [==============================] - 11s 7ms/step - loss: 0.0732 - accuracy: 0.9791 - val_loss: 0.0971 - val_accuracy: 0.9725\n",
      "Epoch 10/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0626 - accuracy: 0.9816 - val_loss: 0.1011 - val_accuracy: 0.9752\n",
      "Epoch 11/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.0593 - accuracy: 0.9839 - val_loss: 0.1071 - val_accuracy: 0.9743\n",
      "Epoch 12/15\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0557 - accuracy: 0.9841 - val_loss: 0.0989 - val_accuracy: 0.9772\n",
      "Epoch 13/15\n",
      "1688/1688 [==============================] - 11s 7ms/step - loss: 0.0509 - accuracy: 0.9853 - val_loss: 0.0864 - val_accuracy: 0.9808\n",
      "Epoch 14/15\n",
      "1688/1688 [==============================] - 11s 7ms/step - loss: 0.0510 - accuracy: 0.9861 - val_loss: 0.1140 - val_accuracy: 0.9710\n",
      "Epoch 15/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0478 - accuracy: 0.9868 - val_loss: 0.1178 - val_accuracy: 0.9760\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(X_train, train_labels,validation_split=0.1, epochs=15,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0d05a8d659e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tensorboard'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'--logdir logs #--port 6006 #--bind_all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorboard\\notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[1;34m(line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorboard\\notebook.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(args_string)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[0mparsed_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[0mstart_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorboard\\manager.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(arguments, timeout)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[0mend_time_seconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_time_seconds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend_time_seconds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll_interval_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[0msubprocess_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs #--port 6006 #--bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000, 784) (10000,)\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1440 - accuracy: 0.9732\n",
      "Test accuracy: 0.9732000231742859\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "test_images_fc = test_images.reshape(-1,784)\n",
    "print(test_images_fc.shape, test_labels.shape)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images_fc, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "To-DFyC6CH8L"
   },
   "source": [
    "## Part 2: Classification of MNIST with Convolutional Neural Networks\n",
    "\n",
    "Next, let's build a convolutional neural network (CNN) classifier to classify images of handwritten digits in the MNIST dataset with a twist where we test our classifier on high-resolution hand-written digits from outside the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "css0a5S-CH8b"
   },
   "source": [
    "We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we divide the values by 255. It's important that the *training set* and the *testing set* are preprocessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p33gNrqJCH8b"
   },
   "outputs": [],
   "source": [
    "def preprocess_images(imgs): # should work for both a single image and multiple images\n",
    "    if imgs.shape != [(28, 28, 1)] and imgs.ndim == 3:\n",
    "        imgs = cv2.cvtColor(imgs, cv2.COLOR_RGB2GRAY)\n",
    "        imgs = cv2.resize(imgs,(28,28))\n",
    "        imgs = cv2.bitwise_not(imgs)\n",
    "    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]\n",
    "    assert sample_img.shape in [(28, 28, 1), (28, 28)], sample_img.shape # make sure images are 28x28 and single-channel (grayscale)\n",
    "    return imgs / 255.0\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "As9XZZqsCH8q"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Now build a very simple Convolutional neural network that looks like this: \n",
    "<img src=\"Pictures/Class_CNN.png\" width=\"700px\" > \n",
    "\n",
    "Try to figure out the filter and the pooling sizes yourself using the picture above. all the activations inside the layers should be 'relu' and the last dense layer again 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRITwgRnCH8r"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "#YOUR TURN\n",
    "# 32 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 64 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "# fully connected to get all relevant data\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# one more dropout\n",
    "# output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Click here for one possible solution</b></summary>\n",
    "    \n",
    "```python\n",
    "# 32 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 64 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "# fully connected to get all relevant data\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# one more dropout\n",
    "# output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuNI0VrlCH8w"
   },
   "source": [
    "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
    "\n",
    "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
    "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
    "* *Metrics* - used to monitor the training and testing steps. \"accuracy\" is the fraction of images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQm1lOmcCH8y"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9D1tzdAwCH85"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Training the neural network model requires the following steps:\n",
    "\n",
    "1. Feed the training data to the model—in this example, the `train_images` and `train_labels` arrays.\n",
    "2. The model learns to associate images and labels.\n",
    "3. We ask the model to make predictions about a test set—in this example, the `test_images` array. We verify that the predictions match the labels from the `test_labels` array. \n",
    "\n",
    "To start training,  call the `model.fit` method—the model is \"fit\" to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-AfdC4CCH86",
    "outputId": "2a9def2b-534f-4928-b220-751197abc686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   2/1875 [..............................] - ETA: 19:36 - loss: 2.2997 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0320s vs `on_train_batch_end` time: 1.2151s). Check your callbacks.\n",
      " 520/1875 [=======>......................] - ETA: 1:08 - loss: 0.2719 - accuracy: 0.9167"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-89987a33c030>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_2.1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=5,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13804), started 0:02:23 ago. (Use '!kill 13804' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eac54ecf5eadc245\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eac54ecf5eadc245\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --port 6006 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUzA1mpKCH8_"
   },
   "source": [
    "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 98.68% on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eehP8cSmCH9B"
   },
   "source": [
    "### Evaluate accuracy\n",
    "\n",
    "Next, compare how the model performs on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGXb-8WGCH9B",
    "outputId": "aaa6ef72-1dc7-4f11-8e1e-3bfbb4ddb5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9903\n",
      "Test accuracy: 0.9902999997138977\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivfsyxzvaE1r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 977    1    0    0    0    0    1    1    0    0]\n",
      " [   0 1133    1    1    0    0    0    0    0    0]\n",
      " [   2    0 1023    1    0    0    0    6    0    0]\n",
      " [   0    0    1 1006    0    2    0    0    1    0]\n",
      " [   0    0    1    0  977    0    4    0    0    0]\n",
      " [   1    0    0    6    0  882    1    0    2    0]\n",
      " [   5    3    0    0    2    1  946    0    1    0]\n",
      " [   0    2    3    1    0    0    0 1020    0    2]\n",
      " [   2    0    2    6    0    1    1    2  957    3]\n",
      " [   0    0    0    4   13    5    0    5    0  982]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(test_images)\n",
    "cm=confusion_matrix(test_labels,np.argmax(y_pred,axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0K4T-7UzCH9J"
   },
   "source": [
    "Often times, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of *overfitting*. In our case, the accuracy is better at 99.19%! This is, in part, due to successful regularization accomplished with the Dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_MGXVbPCH9K"
   },
   "source": [
    "### Make predictions\n",
    "\n",
    "With the model trained, we can use it to make predictions about some images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [0,1,2,3,4,5,6,7,8,9]\n",
    "INCLUDED_LABELS = [0,1,2,3,4,5,6,7,8,9]\n",
    "def plot_image_with_bar(idx, preds, img, with_bar=True, subtitle=''):    \n",
    "    from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "    \n",
    "    \n",
    "    def autolabel(rects,ax):\n",
    "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            perc = height*100\n",
    "            ax.annotate('{:2.2f}%'.format(perc),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    '''Get the values'''\n",
    "    predictions_array = preds\n",
    "    true_label = 1\n",
    "    img = img\n",
    "    #predictions_array, true_label, img = preds['pred'][idx], preds['true_label'][idx], img[idx]\n",
    "    \n",
    "    # Get the current axis\n",
    "    f = plt.gcf()\n",
    "    f.set_size_inches(18.5, 10.5)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    font_size=16\n",
    "\n",
    "    '''Plot the current image'''\n",
    "    ax.imshow(img,cmap='gray')\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    color = 'red'\n",
    "    \n",
    "    #ax.set_title(\"True: {}\".format(LABELS[true_label]),\n",
    "    #                          color='blue', fontsize=font_size)\n",
    "    \n",
    "    ax.set_xlabel(\"Pred: {} {:2.2f}% \".format(INCLUDED_LABELS[predicted_label],\n",
    "                                100*np.max(predictions_array)),\n",
    "                                color=color, fontsize=font_size)\n",
    "    \n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    ''' Create an divider and add the bar to the right'''   \n",
    "    if with_bar:\n",
    "        ax_divider = make_axes_locatable(ax)\n",
    "        cax = ax_divider.append_axes(\"right\", size=\"170%\", pad=\"2%\")\n",
    "        ax.grid(False)\n",
    "        \n",
    "        ''' Plot the distribution'''\n",
    "        pred_plot = cax.bar(INCLUDED_LABELS, predictions_array, color=\"#777777\")\n",
    "        \n",
    "        ''' Here we check the if the indices are correct'''        \n",
    "        pred_plot[predicted_label].set_color('red')       \n",
    "        \n",
    "        cax.set_xticks(np.arange(len(INCLUDED_LABELS)))\n",
    "        cax.set_xticklabels(INCLUDED_LABELS,rotation=45,fontsize=font_size)\n",
    "        cax.set_yticks([])\n",
    "        cax.set_ylim(0.0,1.1)\n",
    "        # add some additional information\n",
    "        if subtitle != '':\n",
    "            subtitle = '\\n'+subtitle\n",
    "\n",
    "        # put the y-values on top of the bar\n",
    "        autolabel(pred_plot,cax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it with your own Image\n",
    "You can now try to predict on your images. Open paint for example and draw a number or something else. You then can read it into the Notebook by putting it in your workshop folder. \n",
    "Keep in mind that we trained our Neural Network on Images with the shape (28,28,1), you might have to resize your own image. (or use the preprocess_images function further up in the script ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = cv2.imread(\"smiley_test.png\")\n",
    "test = preprocess_images(test)\n",
    "\n",
    "# plot the image to check if we read it in correctly\n",
    "plt.imshow(test,cmap='gray')\n",
    "\n",
    "# expanding the dimensions so it fits and prediction with our network\n",
    "test = np.expand_dims(test,axis=2)\n",
    "preds = model.predict(np.expand_dims(test,axis=0))\n",
    "\n",
    "#take the argmax (position of the highest value) from the prediction array\n",
    "plot_image_with_bar(0,preds[0],np.squeeze(test,axis=2))\n",
    "#print(f\"Your picture shows the Number {np.argmax(preds)} with {preds[0,np.argmax(preds)]} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " Workshop tutorial_deep_learning_basics.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb",
     "timestamp": 1601022100465
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
